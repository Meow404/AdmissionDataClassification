{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PartB: Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "from data import readData\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read Data and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readData()\n",
    "data = data / np.array((340, 120, 5, 5, 5, 10, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# random\n",
    "np.random.seed(1234)\n",
    "\n",
    "num_patterns = data.shape[0]\n",
    "num_train_patterns = int(0.7*num_patterns)\n",
    "total_indices = np.arange(num_patterns)\n",
    "\n",
    "np.random.shuffle(total_indices)\n",
    "train_indices = total_indices[:int(0.7*num_patterns)]\n",
    "test_indices = total_indices[int(0.7*num_patterns):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficients sorted in descending order\n",
    "\n",
    "- `CGPA` (5)\n",
    "- `GRE Score` (0)\n",
    "- `TOEFL Score` (1)\n",
    "- `University Rating` (2)\n",
    "- `LOR` (4)\n",
    "- `SOP` (3)\n",
    "- `Research` (6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. All input features (7 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "n = 7 # number of features\n",
    "b = 8 # batch size\n",
    "h = 10 # number of hidden layer neurons\n",
    "regularization_decay = 1e-3\n",
    "# random\n",
    "seed1 = 10\n",
    "seed2 = 20\n",
    "np.random.seed(seed1)\n",
    "tf.random.set_random_seed(seed2)\n",
    "\n",
    "\n",
    "# 3-layer \n",
    "# placeholders\n",
    "x_shape = (None, n) # patterns are row vectors.\n",
    "y_shape = (None, 1) # predicted values are a cell in a column vector.\n",
    "x = tf.placeholder(dtype = tf.float32, shape = x_shape, name = \"x\")\n",
    "y = tf.placeholder(dtype = tf.float32, shape = y_shape, name = \"y\")\n",
    "\n",
    "# weight, biases\n",
    "w1_shape = (n, h)\n",
    "b1_shape = (1, h)\n",
    "w1 = tf.Variable(tf.random.truncated_normal(shape = w1_shape, stddev=1.0 / np.sqrt(n), dtype=tf.float32), name='w1') # taken from start_project1b\n",
    "b1 = tf.Variable(tf.random.normal(shape = b1_shape, dtype = tf.float32), name = \"b1\") # take arbitrary\n",
    "\n",
    "w2_shape = (h, 1)\n",
    "b2_shape = (1, 1)\n",
    "w2 = tf.Variable(tf.random.truncated_normal(shape = w2_shape, stddev = 1.0 / np.sqrt(h), dtype = tf.float32), name = \"w2\")\n",
    "b2 = tf.Variable(tf.random.normal(shape = b2_shape, dtype = tf.float32), name = \"b2\")\n",
    "\n",
    "hidden = tf.nn.relu(tf.linalg.matmul(x, w1) + b1, name = \"hidden\")\n",
    "yp = tf.identity(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "#yp = tf.math.sigmoid(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "\n",
    "# loss + L2 regularization\n",
    "#loss = tf.reduce_mean(tf.math.abs(yp - y))\n",
    "#loss = tf.reduce_mean(tf.math.log(tf.math.cosh(yp - y)))\n",
    "loss = tf.reduce_mean(tf.math.square(yp - y))\n",
    "\n",
    "regularization_loss = tf.reduce_mean(tf.math.square(w1)) + tf.reduce_mean(tf.math.square(w2))\n",
    "total_loss = loss + regularization_decay * regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "x_np = data[:, :-1]\n",
    "y_np = data[:, -1:]\n",
    "print(x_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "x_train = x_np[train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "x_test = x_np[test_indices]\n",
    "y_test = y_np[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, train loss 0.015681631863117218, test loss 0.015380016528069973\n",
      "epoch 200, train loss 0.013160862028598785, test loss 0.013054516166448593\n",
      "epoch 300, train loss 0.01146098505705595, test loss 0.01140685472637415\n",
      "epoch 400, train loss 0.010111178271472454, test loss 0.010077480226755142\n",
      "epoch 500, train loss 0.009031102061271667, test loss 0.009023314341902733\n",
      "epoch 600, train loss 0.008197140879929066, test loss 0.008227636106312275\n",
      "epoch 700, train loss 0.007586782332509756, test loss 0.007667164783924818\n",
      "epoch 800, train loss 0.007157261949032545, test loss 0.007290296256542206\n",
      "epoch 900, train loss 0.006861617788672447, test loss 0.007041470147669315\n",
      "epoch 1000, train loss 0.006656087003648281, test loss 0.006869863253086805\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000\n",
    "\n",
    "# train op\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# start train\n",
    "listtrain_loss = []\n",
    "listtest_loss = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())    \n",
    "for i in range(1, epochs+1, 1):\n",
    "    num_minibatches = 0\n",
    "    for minibatch in range(0, num_train_patterns, b):\n",
    "        sess.run(train_op, feed_dict = {\n",
    "            x: x_train[minibatch:minibatch+b, :],\n",
    "            y: y_train[minibatch:minibatch+b, :],\n",
    "        })\n",
    "        num_minibatches += 1\n",
    "    # each epoch, reshuffle the trainset and record\n",
    "    _indices = np.arange(num_train_patterns)\n",
    "    np.random.shuffle(_indices)\n",
    "    x_train = x_train[_indices]\n",
    "    y_train = y_train[_indices]\n",
    "\n",
    "    train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train})\n",
    "    test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test})\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch {}, train loss {}, test loss {}\".format(i, train_loss, test_loss))\n",
    "    listtrain_loss.append(train_loss)\n",
    "    listtest_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006720219738781452\n"
     ]
    }
   ],
   "source": [
    "# Accuracy (MSE loss)\n",
    "mse = sess.run(loss, feed_dict = {x: x_np, y:y_np})\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove Research ( 6 features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "n = 6 # number of features\n",
    "b = 8 # batch size\n",
    "h = 10 # number of hidden layer neurons\n",
    "regularization_decay = 1e-3\n",
    "# random\n",
    "seed1 = 10\n",
    "seed2 = 20\n",
    "np.random.seed(seed1)\n",
    "tf.random.set_random_seed(seed2)\n",
    "\n",
    "\n",
    "# 3-layer \n",
    "# placeholders\n",
    "x_shape = (None, n) # patterns are row vectors.\n",
    "y_shape = (None, 1) # predicted values are a cell in a column vector.\n",
    "x = tf.placeholder(dtype = tf.float32, shape = x_shape, name = \"x\")\n",
    "y = tf.placeholder(dtype = tf.float32, shape = y_shape, name = \"y\")\n",
    "\n",
    "# weight, biases\n",
    "w1_shape = (n, h)\n",
    "b1_shape = (1, h)\n",
    "w1 = tf.Variable(tf.random.truncated_normal(shape = w1_shape, stddev=1.0 / np.sqrt(n), dtype=tf.float32), name='w1') # taken from start_project1b\n",
    "b1 = tf.Variable(tf.random.normal(shape = b1_shape, dtype = tf.float32), name = \"b1\") # take arbitrary\n",
    "\n",
    "w2_shape = (h, 1)\n",
    "b2_shape = (1, 1)\n",
    "w2 = tf.Variable(tf.random.truncated_normal(shape = w2_shape, stddev = 1.0 / np.sqrt(h), dtype = tf.float32), name = \"w2\")\n",
    "b2 = tf.Variable(tf.random.normal(shape = b2_shape, dtype = tf.float32), name = \"b2\")\n",
    "\n",
    "hidden = tf.nn.relu(tf.linalg.matmul(x, w1) + b1, name = \"hidden\")\n",
    "yp = tf.identity(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "#yp = tf.math.sigmoid(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "\n",
    "# loss + L2 regularization\n",
    "#loss = tf.reduce_mean(tf.math.abs(yp - y))\n",
    "#loss = tf.reduce_mean(tf.math.log(tf.math.cosh(yp - y)))\n",
    "loss = tf.reduce_mean(tf.math.square(yp - y))\n",
    "\n",
    "regularization_loss = tf.reduce_mean(tf.math.square(w1)) + tf.reduce_mean(tf.math.square(w2))\n",
    "total_loss = loss + regularization_decay * regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 6)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "x_np = data[:, :-1]\n",
    "\n",
    "x_np = np.concatenate((x_np[:, :6], x_np[:, 7:]), axis = 1)\n",
    "\n",
    "y_np = data[:, -1:]\n",
    "print(x_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "x_train = x_np[train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "x_test = x_np[test_indices]\n",
    "y_test = y_np[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, train loss 0.008105562999844551, test loss 0.008006610907614231\n",
      "epoch 200, train loss 0.007558065000921488, test loss 0.007460542488843203\n",
      "epoch 300, train loss 0.007240425795316696, test loss 0.007069744635373354\n",
      "epoch 400, train loss 0.007024712860584259, test loss 0.00679671298712492\n",
      "epoch 500, train loss 0.006876887753605843, test loss 0.006566117517650127\n",
      "epoch 600, train loss 0.006748622749000788, test loss 0.006400133948773146\n",
      "epoch 700, train loss 0.0066541763953864574, test loss 0.006287200376391411\n",
      "epoch 800, train loss 0.0065700518898665905, test loss 0.006163093261420727\n",
      "epoch 900, train loss 0.006500947289168835, test loss 0.006073424126952887\n",
      "epoch 1000, train loss 0.006443258374929428, test loss 0.005990808829665184\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000\n",
    "\n",
    "# train op\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# start train\n",
    "listtrain_loss = []\n",
    "listtest_loss = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())    \n",
    "for i in range(1, epochs+1, 1):\n",
    "    num_minibatches = 0\n",
    "    for minibatch in range(0, num_train_patterns, b):\n",
    "        sess.run(train_op, feed_dict = {\n",
    "            x: x_train[minibatch:minibatch+b, :],\n",
    "            y: y_train[minibatch:minibatch+b, :],\n",
    "        })\n",
    "        num_minibatches += 1\n",
    "    # each epoch, reshuffle the trainset and record\n",
    "    _indices = np.arange(num_train_patterns)\n",
    "    np.random.shuffle(_indices)\n",
    "    x_train = x_train[_indices]\n",
    "    y_train = y_train[_indices]\n",
    "\n",
    "    train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train})\n",
    "    test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test})\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch {}, train loss {}, test loss {}\".format(i, train_loss, test_loss))\n",
    "    listtrain_loss.append(train_loss)\n",
    "    listtest_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0063075232319533825\n"
     ]
    }
   ],
   "source": [
    "# Accuracy (MSE loss)\n",
    "mse = sess.run(loss, feed_dict = {x: x_np, y:y_np})\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove Research and SOP ( 5 features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "n = 5 # number of features\n",
    "b = 8 # batch size\n",
    "h = 10 # number of hidden layer neurons\n",
    "regularization_decay = 1e-3\n",
    "# random\n",
    "seed1 = 10\n",
    "seed2 = 20\n",
    "np.random.seed(seed1)\n",
    "tf.random.set_random_seed(seed2)\n",
    "\n",
    "\n",
    "# 3-layer \n",
    "# placeholders\n",
    "x_shape = (None, n) # patterns are row vectors.\n",
    "y_shape = (None, 1) # predicted values are a cell in a column vector.\n",
    "x = tf.placeholder(dtype = tf.float32, shape = x_shape, name = \"x\")\n",
    "y = tf.placeholder(dtype = tf.float32, shape = y_shape, name = \"y\")\n",
    "\n",
    "# weight, biases\n",
    "w1_shape = (n, h)\n",
    "b1_shape = (1, h)\n",
    "w1 = tf.Variable(tf.random.truncated_normal(shape = w1_shape, stddev=1.0 / np.sqrt(n), dtype=tf.float32), name='w1') # taken from start_project1b\n",
    "b1 = tf.Variable(tf.random.normal(shape = b1_shape, dtype = tf.float32), name = \"b1\") # take arbitrary\n",
    "\n",
    "w2_shape = (h, 1)\n",
    "b2_shape = (1, 1)\n",
    "w2 = tf.Variable(tf.random.truncated_normal(shape = w2_shape, stddev = 1.0 / np.sqrt(h), dtype = tf.float32), name = \"w2\")\n",
    "b2 = tf.Variable(tf.random.normal(shape = b2_shape, dtype = tf.float32), name = \"b2\")\n",
    "\n",
    "hidden = tf.nn.relu(tf.linalg.matmul(x, w1) + b1, name = \"hidden\")\n",
    "yp = tf.identity(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "#yp = tf.math.sigmoid(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "\n",
    "# loss + L2 regularization\n",
    "#loss = tf.reduce_mean(tf.math.abs(yp - y))\n",
    "#loss = tf.reduce_mean(tf.math.log(tf.math.cosh(yp - y)))\n",
    "loss = tf.reduce_mean(tf.math.square(yp - y))\n",
    "\n",
    "regularization_loss = tf.reduce_mean(tf.math.square(w1)) + tf.reduce_mean(tf.math.square(w2))\n",
    "total_loss = loss + regularization_decay * regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "x_np = data[:, :-1]\n",
    "\n",
    "x_np = np.concatenate((x_np[:, :3], x_np[:, 4:6], x_np[:, 7:]), axis = 1)\n",
    "\n",
    "y_np = data[:, -1:]\n",
    "print(x_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "x_train = x_np[train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "x_test = x_np[test_indices]\n",
    "y_test = y_np[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, train loss 0.009749854914844036, test loss 0.008739070035517216\n",
      "epoch 200, train loss 0.007932465523481369, test loss 0.006948746740818024\n",
      "epoch 300, train loss 0.0072141001001000404, test loss 0.00628742715343833\n",
      "epoch 400, train loss 0.0069101243279874325, test loss 0.006016381084918976\n",
      "epoch 500, train loss 0.006772094406187534, test loss 0.005915501620620489\n",
      "epoch 600, train loss 0.0066932146437466145, test loss 0.005859160330146551\n",
      "epoch 700, train loss 0.006641136948019266, test loss 0.0058260951191186905\n",
      "epoch 800, train loss 0.006595359183847904, test loss 0.005792982876300812\n",
      "epoch 900, train loss 0.00655570300295949, test loss 0.005763940047472715\n",
      "epoch 1000, train loss 0.006521274335682392, test loss 0.005734381265938282\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000\n",
    "\n",
    "# train op\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# start train\n",
    "listtrain_loss = []\n",
    "listtest_loss = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())    \n",
    "for i in range(1, epochs+1, 1):\n",
    "    num_minibatches = 0\n",
    "    for minibatch in range(0, num_train_patterns, b):\n",
    "        sess.run(train_op, feed_dict = {\n",
    "            x: x_train[minibatch:minibatch+b, :],\n",
    "            y: y_train[minibatch:minibatch+b, :],\n",
    "        })\n",
    "        num_minibatches += 1\n",
    "    # each epoch, reshuffle the trainset and record\n",
    "    _indices = np.arange(num_train_patterns)\n",
    "    np.random.shuffle(_indices)\n",
    "    x_train = x_train[_indices]\n",
    "    y_train = y_train[_indices]\n",
    "\n",
    "    train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train})\n",
    "    test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test})\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch {}, train loss {}, test loss {}\".format(i, train_loss, test_loss))\n",
    "    listtrain_loss.append(train_loss)\n",
    "    listtest_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006285205949097872\n"
     ]
    }
   ],
   "source": [
    "# Accuracy (MSE loss)\n",
    "mse = sess.run(loss, feed_dict = {x: x_np, y:y_np})\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
