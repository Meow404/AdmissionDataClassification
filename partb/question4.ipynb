{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PartB: Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "from data import readData\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readData()\n",
    "data = data / np.array((340, 120, 5, 5, 5, 10, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# random\n",
    "np.random.seed(231535622)\n",
    "\n",
    "num_patterns = data.shape[0]\n",
    "num_train_patterns = int(0.7*num_patterns)\n",
    "total_indices = np.arange(num_patterns)\n",
    "\n",
    "np.random.shuffle(total_indices)\n",
    "train_indices = total_indices[:int(0.7*num_patterns)]\n",
    "test_indices = total_indices[int(0.7*num_patterns):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build 4-layer feed forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Model\n",
    "```\n",
    "n: number of features,\n",
    "b: batch size\n",
    "x: R bxn\n",
    "y: R bx1\n",
    "\n",
    "h: number of hidden neurons\n",
    "\n",
    "hidden1: R bxh = relu(x w1 + b1*)\n",
    "    w1 (weight of hidden neurons): R nxh\n",
    "    b1*: R bxh = 1* b1\n",
    "    b1 (bias of hidden neurons): R 1xh\n",
    "    1* (vector R bx1)\n",
    "\n",
    "hidden2: R bxh = relu(hidden1 w2 + b2*)\n",
    "    w2 (weight of hidden neurons): R nxh\n",
    "    b2*: R bxh = 1* b2\n",
    "    b2 (bias of hidden neurons): R 1xh\n",
    "    1* (vector R bx1)\n",
    "\n",
    "yp (output): R bx1 = linear(hidden2 w3 + b3*)\n",
    "    w3 (weight of output neuron): R hx1\n",
    "    b3*: R bx1 = 1* b3\n",
    "    b3 (bias of output neurons): R 1x1\n",
    "    1* (vector R bx1)\n",
    "\n",
    "\"\"\" We can omit 1* in tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed params\n",
    "n = 7 # number of features\n",
    "b = 8 # batch size\n",
    "h = 50 # number of hidden layer neurons\n",
    "regularization_decay = 1e-3\n",
    "# random\n",
    "seed1 = 10\n",
    "seed2 = 20\n",
    "np.random.seed(seed1)\n",
    "tf.random.set_random_seed(seed2)\n",
    "\n",
    "\n",
    "# 4-layer \n",
    "# placeholders\n",
    "x_shape = (None, n) # patterns are row vectors.\n",
    "y_shape = (None, 1) # predicted values are a cell in a column vector.\n",
    "x = tf.placeholder(dtype = tf.float32, shape = x_shape, name = \"x\")\n",
    "y = tf.placeholder(dtype = tf.float32, shape = y_shape, name = \"y\")\n",
    "drop_rate = tf.placeholder(dtype = tf.float32, shape = (), name = \"drop_rate\")\n",
    "\n",
    "# weight, biases\n",
    "w1_shape = (n, h)\n",
    "b1_shape = (1, h)\n",
    "w1 = tf.Variable(tf.random.truncated_normal(shape = w1_shape, stddev=1.0 / np.sqrt(n), dtype=tf.float32), name='w1') # taken from start_project1b\n",
    "b1 = tf.Variable(tf.random.normal(shape = b1_shape, dtype = tf.float32), name = \"b1\") # take arbitrary\n",
    "\n",
    "w2_shape = (h, h)\n",
    "b2_shape = (1, h)\n",
    "w2 = tf.Variable(tf.random.truncated_normal(shape = w2_shape, stddev=1.0 / np.sqrt(h), dtype=tf.float32), name = 'w2')\n",
    "b2 = tf.Variable(tf.random.normal(shape = b2_shape, dtype = tf.float32), name = \"b2\")\n",
    "\n",
    "w3_shape = (h, 1)\n",
    "b3_shape = (1, 1)\n",
    "w3 = tf.Variable(tf.random.truncated_normal(shape = w3_shape, stddev = 1.0 / np.sqrt(h), dtype = tf.float32), name = \"w3\")\n",
    "b3 = tf.Variable(tf.random.normal(shape = b3_shape, dtype = tf.float32), name = \"b3\")\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.nn.dropout(tf.linalg.matmul(x, w1) + b1, rate = drop_rate), name = \"hidden1\")\n",
    "hidden2 = tf.nn.relu(tf.nn.dropout(tf.linalg.matmul(hidden1, w2) + b2, rate = drop_rate), name = \"hidden2\")\n",
    "yp = tf.identity(tf.linalg.matmul(hidden2, w3) + b3, name = \"yp\")\n",
    "#yp = tf.math.sigmoid(tf.linalg.matmul(hidden, w2) + b2, name = \"yp\")\n",
    "\n",
    "# loss + L2 regularization\n",
    "#loss = tf.reduce_mean(tf.math.abs(yp - y))\n",
    "loss = tf.reduce_mean(tf.math.log(tf.math.cosh(yp - y)))\n",
    "#loss = tf.reduce_mean(tf.math.square(yp - y))\n",
    "\n",
    "regularization_loss = tf.reduce_sum(tf.math.square(w1)) + tf.reduce_sum(tf.math.square(w2))\n",
    "total_loss = loss + regularization_decay * regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "x_np = data[:, :-1]\n",
    "y_np = data[:, -1:]\n",
    "print(x_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "x_train = x_np[train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "x_test = x_np[test_indices]\n",
    "y_test = y_np[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 2.5090484619140625, test loss 2.5116307735443115\n",
      "epoch 100, train loss 0.003061913186684251, test loss 0.002955207135528326\n",
      "epoch 200, train loss 0.002938133431598544, test loss 0.0028392388485372066\n",
      "epoch 300, train loss 0.0028762659057974815, test loss 0.002800253452733159\n",
      "epoch 400, train loss 0.002820969559252262, test loss 0.002762214047834277\n",
      "epoch 500, train loss 0.002778434194624424, test loss 0.0027342785615473986\n",
      "epoch 600, train loss 0.0027479417622089386, test loss 0.0027205045334994793\n",
      "epoch 700, train loss 0.0027116499841213226, test loss 0.002691153669729829\n",
      "epoch 800, train loss 0.0026906246785074472, test loss 0.0026755358558148146\n",
      "epoch 900, train loss 0.0026625324971973896, test loss 0.002657214878126979\n",
      "epoch 1000, train loss 0.002648293972015381, test loss 0.0026470788288861513\n",
      "epoch 1100, train loss 0.002631760435178876, test loss 0.0026355714071542025\n",
      "epoch 1200, train loss 0.0025999057106673717, test loss 0.00261435704305768\n",
      "epoch 1300, train loss 0.00258365529589355, test loss 0.0026042545214295387\n",
      "epoch 1400, train loss 0.0025840771850198507, test loss 0.002599539002403617\n",
      "epoch 1500, train loss 0.0025550834834575653, test loss 0.0025783029850572348\n",
      "epoch 1600, train loss 0.00253976765088737, test loss 0.0025687229353934526\n",
      "epoch 1700, train loss 0.0025270504411309958, test loss 0.0025567689444869757\n",
      "epoch 1800, train loss 0.0025151697918772697, test loss 0.002552141435444355\n",
      "epoch 1900, train loss 0.0025092726573348045, test loss 0.0025368661154061556\n",
      "epoch 2000, train loss 0.0024892801884561777, test loss 0.0025290483608841896\n",
      "epoch 2100, train loss 0.0024787839502096176, test loss 0.0025215228088200092\n",
      "epoch 2200, train loss 0.0024684953968971968, test loss 0.002513403305783868\n",
      "epoch 2300, train loss 0.0024579649325460196, test loss 0.0025005985517054796\n",
      "epoch 2400, train loss 0.002448663115501404, test loss 0.002495820401236415\n",
      "epoch 2500, train loss 0.002451560692861676, test loss 0.002509012585505843\n",
      "epoch 2600, train loss 0.002434731926769018, test loss 0.0024813050404191017\n",
      "epoch 2700, train loss 0.0024253311567008495, test loss 0.0024756768252700567\n",
      "epoch 2800, train loss 0.00241852062754333, test loss 0.0024686907418072224\n",
      "epoch 2900, train loss 0.0024113731924444437, test loss 0.002463016426190734\n",
      "epoch 3000, train loss 0.002410033019259572, test loss 0.002469198312610388\n",
      "epoch 3100, train loss 0.00239781616255641, test loss 0.0024529597721993923\n",
      "epoch 3200, train loss 0.002391357207670808, test loss 0.0024459469132125378\n",
      "epoch 3300, train loss 0.0023856419138610363, test loss 0.0024403377901762724\n",
      "epoch 3400, train loss 0.002386317355558276, test loss 0.0024511071387678385\n",
      "epoch 3500, train loss 0.0023787803947925568, test loss 0.0024437278043478727\n",
      "epoch 3600, train loss 0.002372047631070018, test loss 0.0024374693166464567\n",
      "epoch 3700, train loss 0.002363723935559392, test loss 0.002423141384497285\n",
      "epoch 3800, train loss 0.002362692030146718, test loss 0.002419531112536788\n",
      "epoch 3900, train loss 0.002353030489757657, test loss 0.0024166672956198454\n",
      "epoch 4000, train loss 0.002349462127313018, test loss 0.0024166153743863106\n",
      "epoch 4100, train loss 0.002346230437979102, test loss 0.0024153850972652435\n",
      "epoch 4200, train loss 0.0023402187507599592, test loss 0.0024036825634539127\n",
      "epoch 4300, train loss 0.0023376517929136753, test loss 0.002399981953203678\n",
      "epoch 4400, train loss 0.0023351814597845078, test loss 0.0024065300822257996\n",
      "epoch 4500, train loss 0.002328615402802825, test loss 0.002397912321612239\n",
      "epoch 4600, train loss 0.002322121988981962, test loss 0.0023898163344711065\n",
      "epoch 4700, train loss 0.002315688179805875, test loss 0.0023878703359514475\n",
      "epoch 4800, train loss 0.00230771373026073, test loss 0.0023824230302125216\n",
      "epoch 4900, train loss 0.0023039178922772408, test loss 0.002377317287027836\n",
      "epoch 5000, train loss 0.0023082904517650604, test loss 0.0023755417205393314\n",
      "epoch 5100, train loss 0.0023064124397933483, test loss 0.0023855515755712986\n",
      "epoch 5200, train loss 0.0023031248711049557, test loss 0.002381099620833993\n",
      "epoch 5300, train loss 0.002293854020535946, test loss 0.0023667761124670506\n",
      "epoch 5400, train loss 0.002291884971782565, test loss 0.0023614505771547556\n",
      "epoch 5500, train loss 0.0022950582206249237, test loss 0.0023604014422744513\n",
      "epoch 5600, train loss 0.0023058501537889242, test loss 0.002386464737355709\n",
      "epoch 5700, train loss 0.002301868749782443, test loss 0.002381717087700963\n",
      "epoch 5800, train loss 0.002284546382725239, test loss 0.0023564195726066828\n",
      "epoch 5900, train loss 0.0022843475453555584, test loss 0.002352074021473527\n",
      "epoch 6000, train loss 0.0022813857067376375, test loss 0.002351455856114626\n",
      "epoch 6100, train loss 0.0022803021129220724, test loss 0.002351797418668866\n",
      "epoch 6200, train loss 0.002282686997205019, test loss 0.002348489360883832\n",
      "epoch 6300, train loss 0.0022783575113862753, test loss 0.002346808323636651\n",
      "epoch 6400, train loss 0.0022801607847213745, test loss 0.0023464930709451437\n",
      "epoch 6500, train loss 0.0022756014950573444, test loss 0.0023455966729670763\n",
      "epoch 6600, train loss 0.0022745842579752207, test loss 0.002346600638702512\n",
      "epoch 6700, train loss 0.0022768548224121332, test loss 0.002343997824937105\n",
      "epoch 6800, train loss 0.002290988340973854, test loss 0.00237297429703176\n",
      "epoch 6900, train loss 0.0022732976358383894, test loss 0.0023483633995056152\n",
      "epoch 7000, train loss 0.002280990593135357, test loss 0.0023462362587451935\n",
      "epoch 7100, train loss 0.0022827445063740015, test loss 0.002347436733543873\n",
      "epoch 7200, train loss 0.0022835684940218925, test loss 0.0023658836726099253\n",
      "epoch 7300, train loss 0.0022692272905260324, test loss 0.002342271851375699\n",
      "epoch 7400, train loss 0.0022720308043062687, test loss 0.0023510276805609465\n",
      "epoch 7500, train loss 0.002271374687552452, test loss 0.0023505217395722866\n",
      "epoch 7600, train loss 0.0022680091205984354, test loss 0.00234137917868793\n",
      "epoch 7700, train loss 0.002267005154863, test loss 0.0023430504370480776\n",
      "epoch 7800, train loss 0.002266637282446027, test loss 0.002341414336115122\n",
      "epoch 7900, train loss 0.0022660482209175825, test loss 0.002342336345463991\n",
      "epoch 8000, train loss 0.002265830058604479, test loss 0.002340857870876789\n",
      "epoch 8100, train loss 0.0022652889601886272, test loss 0.0023405381944030523\n",
      "epoch 8200, train loss 0.0022660179529339075, test loss 0.0023396627511829138\n",
      "epoch 8300, train loss 0.002276984043419361, test loss 0.002362326253205538\n",
      "epoch 8400, train loss 0.002267164411023259, test loss 0.0023485859856009483\n",
      "epoch 8500, train loss 0.002263377420604229, test loss 0.0023408725392073393\n",
      "epoch 8600, train loss 0.002263897331431508, test loss 0.00234339595772326\n",
      "epoch 8700, train loss 0.0022708731703460217, test loss 0.0023548423778265715\n",
      "epoch 8800, train loss 0.002268560929223895, test loss 0.0023408764973282814\n",
      "epoch 8900, train loss 0.0022624246776103973, test loss 0.00234170607291162\n",
      "epoch 9000, train loss 0.002263207919895649, test loss 0.002339924918487668\n",
      "epoch 9100, train loss 0.0022645234130322933, test loss 0.0023401970975100994\n",
      "epoch 9200, train loss 0.002262877533212304, test loss 0.002344634151086211\n",
      "epoch 9300, train loss 0.002270965138450265, test loss 0.0023576465900987387\n",
      "epoch 9400, train loss 0.0022696771193295717, test loss 0.0023432921152561903\n",
      "epoch 9500, train loss 0.0022629997693002224, test loss 0.0023412888403981924\n",
      "epoch 9600, train loss 0.0022683730348944664, test loss 0.002343179890885949\n",
      "epoch 9700, train loss 0.0022626554127782583, test loss 0.00234390189871192\n",
      "epoch 9800, train loss 0.0022648966405540705, test loss 0.0023493000771850348\n",
      "epoch 9900, train loss 0.002265890594571829, test loss 0.002351250732317567\n",
      "epoch 10000, train loss 0.002264465671032667, test loss 0.0023482441902160645\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "epochs = 10000\n",
    "dropout = 0.0\n",
    "\n",
    "# train op\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "sess = tf.Session()\n",
    "\n",
    "# start train\n",
    "listtrain_loss = []\n",
    "listtest_loss = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train, drop_rate: dropout})\n",
    "test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test, drop_rate: dropout})\n",
    "print(\"epoch 0, train loss {}, test loss {}\".format(train_loss, test_loss))\n",
    "\n",
    "for i in range(1, epochs+1, 1):\n",
    "    for minibatch in range(0, num_train_patterns, b):\n",
    "        sess.run(train_op, feed_dict = {\n",
    "            x: x_train[minibatch:minibatch+b, :],\n",
    "            y: y_train[minibatch:minibatch+b, :],\n",
    "            drop_rate: dropout\n",
    "        })\n",
    "    # each epoch, reshuffle the trainset and record\n",
    "    _indices = np.arange(num_train_patterns)\n",
    "    np.random.shuffle(_indices)\n",
    "    x_train = x_train[_indices]\n",
    "    y_train = y_train[_indices]\n",
    "    \n",
    "    train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train, drop_rate: 0})\n",
    "    test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test, drop_rate: 0})\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch {}, train loss {}, test loss {}\".format(i, train_loss, test_loss))\n",
    "    listtrain_loss.append(train_loss)\n",
    "    listtest_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Plot train errors and test errors against epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f861c23d910>,\n",
       " <matplotlib.lines.Line2D at 0x7f861c23dad0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x777.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12.8, 10.8))\n",
    "plt.plot(range(epochs), listtrain_loss, \"bs\", range(epochs), listtest_loss, \"g^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "x_np = data[:, :-1]\n",
    "y_np = data[:, -1:]\n",
    "print(x_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "x_train = x_np[train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "x_test = x_np[test_indices]\n",
    "y_test = y_np[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 2.567216396331787, test loss 2.498887300491333\n",
      "epoch 100, train loss 0.009551732800900936, test loss 0.009316802024841309\n",
      "epoch 200, train loss 0.010277172550559044, test loss 0.010168185457587242\n",
      "epoch 300, train loss 0.00783125963062048, test loss 0.007783896755427122\n",
      "epoch 400, train loss 0.009826701134443283, test loss 0.00985423568636179\n",
      "epoch 500, train loss 0.0060454136691987514, test loss 0.0060804313980042934\n",
      "epoch 600, train loss 0.006003713700920343, test loss 0.006053350865840912\n",
      "epoch 700, train loss 0.0067449952475726604, test loss 0.0068260845728218555\n",
      "epoch 800, train loss 0.005594135262072086, test loss 0.005690972786396742\n",
      "epoch 900, train loss 0.007102084346115589, test loss 0.007248677778989077\n",
      "epoch 1000, train loss 0.008210742846131325, test loss 0.008376741781830788\n",
      "epoch 1100, train loss 0.006946654990315437, test loss 0.00709911435842514\n",
      "epoch 1200, train loss 0.00553267914801836, test loss 0.005654771346598864\n",
      "epoch 1300, train loss 0.004915859550237656, test loss 0.005026382394134998\n",
      "epoch 1400, train loss 0.005592705681920052, test loss 0.005698103923350573\n",
      "epoch 1500, train loss 0.004979847464710474, test loss 0.0050804028287529945\n",
      "epoch 1600, train loss 0.004761753138154745, test loss 0.004860694520175457\n",
      "epoch 1700, train loss 0.004792605992406607, test loss 0.00488383648917079\n",
      "epoch 1800, train loss 0.004470685496926308, test loss 0.004566796589642763\n",
      "epoch 1900, train loss 0.004498633090406656, test loss 0.004588673356920481\n",
      "epoch 2000, train loss 0.005011998116970062, test loss 0.0051003932021558285\n",
      "epoch 2100, train loss 0.004511886276304722, test loss 0.004604055546224117\n",
      "epoch 2200, train loss 0.004118661396205425, test loss 0.004210856277495623\n",
      "epoch 2300, train loss 0.004169801715761423, test loss 0.004260045476257801\n",
      "epoch 2400, train loss 0.004378574434667826, test loss 0.004466974642127752\n",
      "epoch 2500, train loss 0.004270252771675587, test loss 0.00436008907854557\n",
      "epoch 2600, train loss 0.00447608157992363, test loss 0.004561221692711115\n",
      "epoch 2700, train loss 0.0045255389995872974, test loss 0.004605458118021488\n",
      "epoch 2800, train loss 0.0044257366098463535, test loss 0.004512691404670477\n",
      "epoch 2900, train loss 0.004362836014479399, test loss 0.004447103478014469\n",
      "epoch 3000, train loss 0.004521511495113373, test loss 0.004603015724569559\n",
      "epoch 3100, train loss 0.004258216358721256, test loss 0.004343018867075443\n",
      "epoch 3200, train loss 0.0042818584479391575, test loss 0.004368837457150221\n",
      "epoch 3300, train loss 0.004138741176575422, test loss 0.004223004914820194\n",
      "epoch 3400, train loss 0.004420498851686716, test loss 0.004505788441747427\n",
      "epoch 3500, train loss 0.0042112963274121284, test loss 0.004295832943171263\n",
      "epoch 3600, train loss 0.0042191240936517715, test loss 0.004300274420529604\n",
      "epoch 3700, train loss 0.004323484841734171, test loss 0.004403679631650448\n",
      "epoch 3800, train loss 0.00416024774312973, test loss 0.004237974528223276\n",
      "epoch 3900, train loss 0.004247758537530899, test loss 0.004328616429120302\n",
      "epoch 4000, train loss 0.004281484987586737, test loss 0.004359654616564512\n",
      "epoch 4100, train loss 0.004228750243782997, test loss 0.0043068998493254185\n",
      "epoch 4200, train loss 0.004220279399305582, test loss 0.004294718150049448\n",
      "epoch 4300, train loss 0.004240165464580059, test loss 0.0043100942857563496\n",
      "epoch 4400, train loss 0.004146580118685961, test loss 0.004227155353873968\n",
      "epoch 4500, train loss 0.004326460417360067, test loss 0.0044046444818377495\n",
      "epoch 4600, train loss 0.004362229723483324, test loss 0.004442882724106312\n",
      "epoch 4700, train loss 0.004257865250110626, test loss 0.004336902871727943\n",
      "epoch 4800, train loss 0.0042094215750694275, test loss 0.004295079968869686\n",
      "epoch 4900, train loss 0.004165303893387318, test loss 0.004256183747202158\n",
      "epoch 5000, train loss 0.004231843166053295, test loss 0.004319679923355579\n",
      "epoch 5100, train loss 0.004239295143634081, test loss 0.004331828095018864\n",
      "epoch 5200, train loss 0.004128591623157263, test loss 0.004228259902447462\n",
      "epoch 5300, train loss 0.0042039090767502785, test loss 0.004306284710764885\n",
      "epoch 5400, train loss 0.0041520497761666775, test loss 0.0042581697925925255\n",
      "epoch 5500, train loss 0.004258360713720322, test loss 0.004369014874100685\n",
      "epoch 5600, train loss 0.00416758144274354, test loss 0.004284163471311331\n",
      "epoch 5700, train loss 0.004103914834558964, test loss 0.004223568364977837\n",
      "epoch 5800, train loss 0.004077491816133261, test loss 0.004203057382255793\n",
      "epoch 5900, train loss 0.004129501525312662, test loss 0.0042586117051541805\n",
      "epoch 6000, train loss 0.004256850574165583, test loss 0.004388745408505201\n",
      "epoch 6100, train loss 0.004109639674425125, test loss 0.004243906121701002\n",
      "epoch 6200, train loss 0.004123893100768328, test loss 0.004259071778506041\n",
      "epoch 6300, train loss 0.004073030781000853, test loss 0.004210911225527525\n",
      "epoch 6400, train loss 0.0040120286867022514, test loss 0.004155907314270735\n",
      "epoch 6500, train loss 0.00403635436668992, test loss 0.00418118154630065\n",
      "epoch 6600, train loss 0.004113419447094202, test loss 0.004259628243744373\n",
      "epoch 6700, train loss 0.004118591081351042, test loss 0.004266148898750544\n",
      "epoch 6800, train loss 0.00415171030908823, test loss 0.004307574592530727\n",
      "epoch 6900, train loss 0.004025797359645367, test loss 0.004177493508905172\n",
      "epoch 7000, train loss 0.00402344623580575, test loss 0.0041825189255177975\n",
      "epoch 7100, train loss 0.0042250133119523525, test loss 0.004392522852867842\n",
      "epoch 7200, train loss 0.00407960033044219, test loss 0.004244985058903694\n",
      "epoch 7300, train loss 0.004144280217587948, test loss 0.004314795136451721\n",
      "epoch 7400, train loss 0.004173060413450003, test loss 0.0043473741970956326\n",
      "epoch 7500, train loss 0.0039649782702326775, test loss 0.004129843786358833\n",
      "epoch 7600, train loss 0.004156647250056267, test loss 0.004334427881985903\n",
      "epoch 7700, train loss 0.004158570431172848, test loss 0.004335110541433096\n",
      "epoch 7800, train loss 0.00415709288790822, test loss 0.004338723607361317\n",
      "epoch 7900, train loss 0.004114693496376276, test loss 0.0042915139347314835\n",
      "epoch 8000, train loss 0.0039976523257792, test loss 0.004176916088908911\n",
      "epoch 8100, train loss 0.00406170217320323, test loss 0.0042355298064649105\n",
      "epoch 8200, train loss 0.004150508902966976, test loss 0.004339318256825209\n",
      "epoch 8300, train loss 0.004099540878087282, test loss 0.004285333212465048\n",
      "epoch 8400, train loss 0.004071161616593599, test loss 0.004252116661518812\n",
      "epoch 8500, train loss 0.0041385069489479065, test loss 0.004332412965595722\n",
      "epoch 8600, train loss 0.004092568065971136, test loss 0.004285446368157864\n",
      "epoch 8700, train loss 0.004255378153175116, test loss 0.004456758499145508\n",
      "epoch 8800, train loss 0.0041363537311553955, test loss 0.004331475589424372\n",
      "epoch 8900, train loss 0.004164103884249926, test loss 0.004359633196145296\n",
      "epoch 9000, train loss 0.004127551801502705, test loss 0.004325121641159058\n",
      "epoch 9100, train loss 0.004089207388460636, test loss 0.004279993940144777\n",
      "epoch 9200, train loss 0.0041829729452729225, test loss 0.004388138651847839\n",
      "epoch 9300, train loss 0.004162106197327375, test loss 0.004364470485597849\n",
      "epoch 9400, train loss 0.00420543784275651, test loss 0.004413526970893145\n",
      "epoch 9500, train loss 0.004124798811972141, test loss 0.004322543740272522\n",
      "epoch 9600, train loss 0.0041068559512495995, test loss 0.004297176375985146\n",
      "epoch 9700, train loss 0.0040853447280824184, test loss 0.004277532920241356\n",
      "epoch 9800, train loss 0.00417548231780529, test loss 0.004378564190119505\n",
      "epoch 9900, train loss 0.004053899087011814, test loss 0.004246690776199102\n",
      "epoch 10000, train loss 0.004116587340831757, test loss 0.004310683347284794\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "epochs = 10000\n",
    "dropout = 0.2\n",
    "\n",
    "# train op\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "sess = tf.Session()\n",
    "\n",
    "# start train\n",
    "listtrain_loss = []\n",
    "listtest_loss = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train, drop_rate: dropout})\n",
    "test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test, drop_rate: dropout})\n",
    "print(\"epoch 0, train loss {}, test loss {}\".format(train_loss, test_loss))\n",
    "\n",
    "for i in range(1, epochs+1, 1):\n",
    "    for minibatch in range(0, num_train_patterns, b):\n",
    "        sess.run(train_op, feed_dict = {\n",
    "            x: x_train[minibatch:minibatch+b, :],\n",
    "            y: y_train[minibatch:minibatch+b, :],\n",
    "            drop_rate: dropout\n",
    "        })\n",
    "    # each epoch, reshuffle the trainset and record\n",
    "    _indices = np.arange(num_train_patterns)\n",
    "    np.random.shuffle(_indices)\n",
    "    x_train = x_train[_indices]\n",
    "    y_train = y_train[_indices]\n",
    "    \n",
    "    train_loss = sess.run(loss, feed_dict = {x: x_train, y:y_train, drop_rate: 0})\n",
    "    test_loss = sess.run(loss, feed_dict = {x: x_test, y:y_test, drop_rate: 0})\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch {}, train loss {}, test loss {}\".format(i, train_loss, test_loss))\n",
    "    listtrain_loss.append(train_loss)\n",
    "    listtest_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Plot train errors and test errors against epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f861c288cd0>,\n",
       " <matplotlib.lines.Line2D at 0x7f861c288ed0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x777.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12.8, 10.8))\n",
    "plt.plot(range(epochs), listtrain_loss, \"bs\", range(epochs), listtest_loss, \"g^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
